from ast import Str
from math import fabs
from operator import length_hint
from subprocess import Popen
import sqlite3
import csv
import asyncio
import websockets
import json
import tldextract
import sys
import time
import argparse
import os
from multiprocessing import Process

'''
run command:

python3 exploit_scanner.py --database test.db --exploits vulnerability_urls.txt  --debug

This backend crawler listens for a conneciton from chrome on port localhost:8787.
It then waits to receive a start message from chrome like so:
{"start_crawl": 1}
After that it starts sending urls to chrome in a json format like so (a single url per message):
{"url": "www.google.com"}
and receives from chrome replies ("links", "flow" and "close" are optional per message):
{ 
  "url": "www.google.com",
  "links": ["www.google.com/link1", "www.google.com/link2", "www.google.com/link3"]
  "flow": "......" 
  "close": 1     // this means that chrome finished processing the url and retrieved all links and all flows for this url. This must be sent eventually for every url!
}
'''

DOMAIN_LIMIT = 1975 #the number of exploits to execute from exploit_urls.txt
PORT = 8787
websocket_to_chrome = None
outstanding_urls = 0
MAX_OUTSTANDING_URLS = 15 #number of opened tabs in chrome
debug = False
con = None # database connection
urls_to_send = []

def log(msg):
    global debug 
    if debug:
        print(msg)
        
no_fetch_extract = tldextract.TLDExtract(suffix_list_urls=["file:///public_suffix_list.dat"])
        
def open_db(database_file):
    con = sqlite3.connect(database_file)
    cur = con.cursor()
    # check if database is new
    res = cur.execute("SELECT name FROM sqlite_master")
    log(f"Database {database_file} is open.")
    if res.fetchone() is not None: #not new
        log(f"Database is not new.")
        return con
    log(f"Database is new.")
    #initialize the tables
    cur.execute("CREATE TABLE urls(domain TEXT, url TEXT PRIMARY KEY, depth INTEGER, scanned INTEGER, is_exploit INTEGER)")
    cur.execute("CREATE TABLE flows(url TEXT, hash TEXT PRIMARY KEY)")
    log(f"Tables created.")
    return con


def seed_db(con, domains):
    log(f"Seeding db.") 
    cur = con.cursor()
    with open(domains) as csvfile:
        domainreader = csv.reader(csvfile)
        data = []
        for row in domainreader:
            if int(row[0]) > DOMAIN_LIMIT:
                break
                
            ext = no_fetch_extract(row[1])
            row_link_domain = ".".join([ext.domain, ext.suffix])
            log(f"the link domain is: " + row_link_domain);
            
            data.append((row_link_domain, row[1], 0, 0, 0))
            log(f"Found {row[1]} is domain seed file.")
            
        
        
        cur.executemany("INSERT OR IGNORE INTO urls VALUES(?, ?, ?, ?, ?)", data)
        con.commit()
        log(f"Database seeded with domains.")

def fix_url(url): #url is a string
    log("fixing url tho")
    url.replace("'", "\\'")
    url.replace('"', '\\"')
    return url

def mark_scanned_url(con, cur, url):
    log(f"mark the url: {url} as scanned")
    #url = fix_url(url)
    #log(url)
    log(url)
    res = cur.execute("SELECT domain, depth, scanned, is_exploit FROM urls WHERE url=(?)", (url,))
    url_entry = res.fetchall()
    log(len(url_entry))
    assert(len(url_entry) == 1) #checks that there is only one instance of this url(it means that I 
                                # have to check in the back script that there is no double urls
    (url_domain, url_depth, url_scanned, url_is_exploit) = url_entry[0]
    #Mark the url as scanned
    if not url_scanned:
        log(f"Mark {url} as scanned.")
        cur.execute("UPDATE urls SET scanned = 1 WHERE url=(?)", (url,))
        log("marked the url: {url} as scanned")
        con.commit()




async def send_urls(websocket, cur):
    global outstanding_urls
    global urls_to_send
    #takes the urls from the database and stores them in urls_to_send. note: urls_to_send is always of 
    #size 1 because of the return false in the end
    while outstanding_urls < MAX_OUTSTANDING_URLS:
        if len(urls_to_send) == 0:
            log("No more urls. Fetching new ones from db.")
            res = cur.execute(f"SELECT url FROM urls WHERE scanned = 0")
            urls_to_send.extend(res.fetchall())
            log(f"Fetched {len(urls_to_send)} urls.")
            #log(urls_to_send)
            if len(urls_to_send) == 0:
                log("No more urls to crawl. Exiting.")
                return True
        url = urls_to_send.pop()
        log(f"Sending {url}....")
        msg = ''
        
        msg = {'url': str(url[0]) }
        msg = json.dumps(msg)

        await websocket.send(msg)
        #log(f"Sent {msg}.")
        outstanding_urls = outstanding_urls + 1
        log(f"Outstanding urls {outstanding_urls}.")
    return False
    

async def process_start(websocket, cur):
    global websocket_to_chrome
    websocket_to_chrome = websocket
    log(f"Got start.")
    await send_urls(websocket, cur)

def exploit_url(con, cur, url):
    log(f"process exploit")
    log(url)
    res = cur.execute("SELECT domain, depth, scanned, is_exploit FROM urls WHERE url=(?)", (url,))
    url_entry = res.fetchall()
    assert(len(url_entry) == 1) #checks that there is only one instance of this url
    (url_domain, url_depth, url_scanned, url_is_exploit) = url_entry[0]
    #Mark the url as scanned
    if not url_scanned:
        log(f"EXPLOIT {url} as EXPLOIT.")
        cur.execute("UPDATE urls SET is_exploit = 1 WHERE url=(?)", (url,))
        log("EXPLOIT the url: {url} as EXPLOIT")
        con.commit()

async def process_message(websocket):
    global outstanding_urls
    global con
    cur = con.cursor()
    async for message in websocket:
        #log(f"Got message {message}.") # we dont display the actual message because it can be long
        m = json.loads(message)
        if "start_crawl" in m:
            await process_start(websocket, cur)
            continue
        url = m["url"]
        if "ping" in m: #because apperently the websocket decides to just close for no reason when it doesnt recieve any messages
            log("got ping")   
            continue           
        log(f"Got message from url: {url}"); 
        if "exploit" in m:
            log(f"found exploit for: {url}")
            exploit_url(con, cur, url)

        if "close" in m:
            mark_scanned_url(con, cur, url) #mark the url as scanned
            log(f"Got url close from url: {url}")
            outstanding_urls = outstanding_urls - 1 
            log(f"Outstanding urls {outstanding_urls}.")
            stop = await send_urls(websocket, cur)
            if stop:
                return
        
           

#waiting for a connection from chrome via websocket
#to connect open chrome and start the exploit_extension
#use localhost address if you run this program and chrome on the same machine. if not, use the host ip address, and dont forget to update this address in the backscript of the exploit_extension
async def websocket_server():
    async with websockets.serve(process_message, "localhost", PORT, ping_interval=None, max_size = 100000000000):
        await asyncio.Future()  # run forever


def main(): 
    # Launch chrome
    parser = argparse.ArgumentParser()
    parser.add_argument('--database', type=str, required=True,
                        help='Enter database (sqllite) filename that stores all content')
    parser.add_argument('--exploits', type=str, required=True,
                        help='Enter the path to the file that contains the domains to crawl')
    #parser.add_argument('--chrome', type=str, required=True, help='Path to chrome')
    parser.add_argument('--debug', dest='debug', action='store_true')
    args = parser.parse_args()
    
    global debug
    debug = args.debug
      

    # Init DB
    global con
    con = open_db(args.database)
    cur = con.cursor()
    seed_db(con, args.exploits)
    
    log(f"Starting websocket server.")
    #Launch websocket server
    asyncio.run(websocket_server())
 
    
 
 
if __name__ == '__main__':
    main()
